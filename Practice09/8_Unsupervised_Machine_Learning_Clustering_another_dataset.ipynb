{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16f70cc7",
   "metadata": {},
   "source": [
    "# Prepare the environment and collect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa72d595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/18 15:26:46 WARN Utils: Your hostname, m0 resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "22/10/18 15:26:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/18 15:26:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/10/18 15:27:03 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/10/18 15:27:03 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/10/18 15:27:03 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "22/10/18 15:27:03 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "22/10/18 15:27:03 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: string (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- default: string (nullable = true)\n",
      " |-- balance: string (nullable = true)\n",
      " |-- housing: string (nullable = true)\n",
      " |-- loan: string (nullable = true)\n",
      " |-- contact: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- duration: string (nullable = true)\n",
      " |-- campaign: string (nullable = true)\n",
      " |-- pdays: string (nullable = true)\n",
      " |-- previous: string (nullable = true)\n",
      " |-- poutcome: string (nullable = true)\n",
      " |-- deposit: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"Linear regression\").getOrCreate()\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import StringIndexer, IndexToString\n",
    "from pyspark.ml.feature import VectorAssembler, VectorIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.sql.functions import mean, col\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "\n",
    "file_location = \"bank_deposit.csv\"\n",
    "file_type = \"csv\"\n",
    "infer_schema = \"False\"\n",
    "first_row_is_header = \"True\"\n",
    "df = spark.read.format(file_type) \\\n",
    ".option(\"inferSchema\", infer_schema) \\\n",
    ".option(\"header\", first_row_is_header) \\\n",
    ".load(file_location)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f668d544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-----+\n",
      "|            features|indexedLabel|label|\n",
      "+--------------------+------------+-----+\n",
      "|(30,[3,11,13,16,1...|         1.0|  yes|\n",
      "|(30,[3,11,13,16,1...|         1.0|  yes|\n",
      "|(30,[2,11,13,16,1...|         1.0|  yes|\n",
      "|(30,[4,11,13,16,1...|         1.0|  yes|\n",
      "|(30,[3,11,14,16,1...|         1.0|  yes|\n",
      "|(30,[0,12,14,16,2...|         1.0|  yes|\n",
      "|(30,[0,11,14,16,2...|         1.0|  yes|\n",
      "|(30,[5,13,16,18,2...|         1.0|  yes|\n",
      "|(30,[2,11,13,16,1...|         1.0|  yes|\n",
      "|(30,[4,12,13,16,1...|         1.0|  yes|\n",
      "+--------------------+------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d452fc08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('age', 'int'),\n",
       " ('job', 'string'),\n",
       " ('marital', 'string'),\n",
       " ('education', 'string'),\n",
       " ('default', 'string'),\n",
       " ('balance', 'int'),\n",
       " ('housing', 'string'),\n",
       " ('loan', 'string'),\n",
       " ('contact', 'string'),\n",
       " ('day', 'string'),\n",
       " ('month', 'string'),\n",
       " ('duration', 'int'),\n",
       " ('campaign', 'int'),\n",
       " ('pdays', 'int'),\n",
       " ('previous', 'int'),\n",
       " ('poutcome', 'string'),\n",
       " ('deposit', 'string')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "#Identifying and assigning lists of variables\n",
    "float_vars=['age', 'balance', 'duration','campaign','pdays','previous']\n",
    "#Converting variables\n",
    "for column in float_vars:\n",
    " df=df.withColumn(column,df[column].cast(IntegerType()))\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2507759",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63b222e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_transformation(df, CatCols, continuousCols, labelCol):\n",
    "  \n",
    "  indexers = [StringIndexer(inputCol=c, \n",
    "                            outputCol=\"{0}_indexed\".format(c)) for c in CatCols]\n",
    "\n",
    "  encoders = [OneHotEncoder(inputCol=indexer.getOutputCol(),\n",
    "              outputCol=\"{0}_encoded\".format(indexer.getOutputCol()))\n",
    "              for indexer in indexers]\n",
    "\n",
    "\n",
    "  v = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders]\n",
    "                              + continuousCols, outputCol=\"features\")\n",
    "  \n",
    "  indexer = StringIndexer(inputCol=labelCol, outputCol='indexedLabel')\n",
    "\n",
    "  pipeline = Pipeline(stages = indexers + encoders + [v ] + [indexer])\n",
    "\n",
    "  model=pipeline.fit(df)\n",
    "    \n",
    "  data = model.transform(df)\n",
    "\n",
    "  data =  data.withColumn('label', col(labelCol))\n",
    "  \n",
    "  return  data.select('features', \n",
    "                     'indexedLabel', \n",
    "                     'label'), StringIndexer(inputCol='label').fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b917726a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/18 15:27:57 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 32:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-----+\n",
      "|            features|indexedLabel|label|\n",
      "+--------------------+------------+-----+\n",
      "|(30,[3,11,13,16,1...|         1.0|  yes|\n",
      "|(30,[3,11,13,16,1...|         1.0|  yes|\n",
      "|(30,[2,11,13,16,1...|         1.0|  yes|\n",
      "|(30,[4,11,13,16,1...|         1.0|  yes|\n",
      "|(30,[3,11,14,16,1...|         1.0|  yes|\n",
      "|(30,[0,12,14,16,2...|         1.0|  yes|\n",
      "|(30,[0,11,14,16,2...|         1.0|  yes|\n",
      "|(30,[5,13,16,18,2...|         1.0|  yes|\n",
      "|(30,[2,11,13,16,1...|         1.0|  yes|\n",
      "|(30,[4,12,13,16,1...|         1.0|  yes|\n",
      "+--------------------+------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CatCols = ['job', 'marital', 'education', \n",
    "                      'default', 'housing', 'loan', \n",
    "                      'contact', 'poutcome']\n",
    "\n",
    "NumCols = ['age', 'balance', 'duration', \n",
    "               'campaign', 'pdays', 'previous']\n",
    "\n",
    "(df, labelindexer) = data_transformation(df, CatCols, NumCols, 'deposit')\n",
    "\n",
    "df.show(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b70777dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-----+--------------------+\n",
      "|            features|indexedLabel|label|     indexedFeatures|\n",
      "+--------------------+------------+-----+--------------------+\n",
      "|(30,[3,11,13,16,1...|         1.0|  yes|(30,[3,11,13,16,1...|\n",
      "|(30,[3,11,13,16,1...|         1.0|  yes|(30,[3,11,13,16,1...|\n",
      "|(30,[2,11,13,16,1...|         1.0|  yes|(30,[2,11,13,16,1...|\n",
      "|(30,[4,11,13,16,1...|         1.0|  yes|(30,[4,11,13,16,1...|\n",
      "|(30,[3,11,14,16,1...|         1.0|  yes|(30,[3,11,14,16,1...|\n",
      "|(30,[0,12,14,16,2...|         1.0|  yes|(30,[0,12,14,16,2...|\n",
      "|(30,[0,11,14,16,2...|         1.0|  yes|(30,[0,11,14,16,2...|\n",
      "|(30,[5,13,16,18,2...|         1.0|  yes|(30,[5,13,16,18,2...|\n",
      "|(30,[2,11,13,16,1...|         1.0|  yes|(30,[2,11,13,16,1...|\n",
      "|(30,[4,12,13,16,1...|         1.0|  yes|(30,[4,12,13,16,1...|\n",
      "+--------------------+------------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featureIndexer = VectorIndexer(inputCol=\"features\", \n",
    "                               outputCol=\"indexedFeatures\", \n",
    "                               maxCategories=4).fit(df)\n",
    "\n",
    "featureIndexer.transform(df).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c78e762e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-----+\n",
      "|            features|indexedLabel|label|\n",
      "+--------------------+------------+-----+\n",
      "|(30,[3,11,13,16,1...|         1.0|  yes|\n",
      "|(30,[3,11,13,16,1...|         1.0|  yes|\n",
      "|(30,[2,11,13,16,1...|         1.0|  yes|\n",
      "|(30,[4,11,13,16,1...|         1.0|  yes|\n",
      "|(30,[3,11,14,16,1...|         1.0|  yes|\n",
      "|(30,[0,12,14,16,2...|         1.0|  yes|\n",
      "|(30,[0,11,14,16,2...|         1.0|  yes|\n",
      "|(30,[5,13,16,18,2...|         1.0|  yes|\n",
      "|(30,[2,11,13,16,1...|         1.0|  yes|\n",
      "|(30,[4,12,13,16,1...|         1.0|  yes|\n",
      "+--------------------+------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ccb8023",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 8911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Dataset Count: 2251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 42:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-----+\n",
      "|            features|indexedLabel|label|\n",
      "+--------------------+------------+-----+\n",
      "|(30,[0,11,13,16,1...|         1.0|  yes|\n",
      "|(30,[0,11,13,16,1...|         0.0|   no|\n",
      "|(30,[0,11,13,16,1...|         1.0|  yes|\n",
      "|(30,[0,11,13,16,1...|         0.0|   no|\n",
      "|(30,[0,11,13,16,1...|         1.0|  yes|\n",
      "|(30,[0,11,13,16,1...|         0.0|   no|\n",
      "|(30,[0,11,13,16,1...|         1.0|  yes|\n",
      "|(30,[0,11,13,16,1...|         1.0|  yes|\n",
      "|(30,[0,11,13,16,1...|         0.0|   no|\n",
      "|(30,[0,11,13,16,1...|         0.0|   no|\n",
      "|(30,[0,11,13,16,1...|         0.0|   no|\n",
      "|(30,[0,11,13,16,1...|         1.0|  yes|\n",
      "|(30,[0,11,13,16,1...|         0.0|   no|\n",
      "|(30,[0,11,13,16,1...|         0.0|   no|\n",
      "|(30,[0,11,13,16,1...|         1.0|  yes|\n",
      "|(30,[0,11,13,16,1...|         0.0|   no|\n",
      "|(30,[0,11,13,16,1...|         0.0|   no|\n",
      "|(30,[0,11,13,16,1...|         0.0|   no|\n",
      "|(30,[0,11,13,16,1...|         0.0|   no|\n",
      "|(30,[0,11,13,16,1...|         0.0|   no|\n",
      "+--------------------+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Data splitting\n",
    "(trainingData, testData) = df.randomSplit([0.8, 0.2], seed=10)\n",
    "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
    "print(\"Test Dataset Count: \" + str(testData.count()))\n",
    "trainingData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067d5b9b",
   "metadata": {},
   "source": [
    "# K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4d7df88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 262:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette measure using squared Euclidean distance = 0.877766399738675\n",
      "--*----*----*----*----*----*----*----*----*----*----*----*----*----*----*----*----*----*----*----*--\n",
      "[array([3.37662338e-01, 9.09090909e-02, 1.29870130e-01, 5.19480519e-02,\n",
      "       3.89610390e-02, 1.68831169e-01, 6.49350649e-02, 2.59740260e-02,\n",
      "       0.00000000e+00, 5.19480519e-02, 1.29870130e-02, 6.36363636e-01,\n",
      "       2.85714286e-01, 3.24675325e-01, 5.19480519e-01, 1.03896104e-01,\n",
      "       1.00000000e+00, 6.75324675e-01, 9.61038961e-01, 7.40259740e-01,\n",
      "       1.42857143e-01, 7.27272727e-01, 6.49350649e-02, 1.03896104e-01,\n",
      "       4.73376623e+01, 2.74114935e+04, 3.71519481e+02, 2.88311688e+00,\n",
      "       4.41948052e+01, 9.09090909e-01]), array([2.22458485e-01, 1.79323613e-01, 1.63021466e-01, 1.22620494e-01,\n",
      "       8.63710004e-02, 6.44997975e-02, 3.55407047e-02, 3.24017821e-02,\n",
      "       3.33130822e-02, 2.96678817e-02, 2.46051033e-02, 5.64196031e-01,\n",
      "       3.18550020e-01, 5.03645200e-01, 3.19360065e-01, 1.33151073e-01,\n",
      "       9.83090320e-01, 5.17820980e-01, 8.59963548e-01, 7.21040907e-01,\n",
      "       2.14560551e-01, 7.50303767e-01, 1.10064804e-01, 9.26488457e-02,\n",
      "       4.08314095e+01, 7.37151985e+02, 3.68813285e+02, 2.52126367e+00,\n",
      "       5.12187120e+01, 8.11867153e-01]), array([2.83705542e-01, 1.37303557e-01, 1.67907361e-01, 9.84284533e-02,\n",
      "       5.54177006e-02, 1.05872622e-01, 4.05293631e-02, 3.14309347e-02,\n",
      "       2.31596361e-02, 2.56410256e-02, 2.48138958e-02, 6.03804797e-01,\n",
      "       2.89495451e-01, 3.94540943e-01, 4.09429280e-01, 1.46401985e-01,\n",
      "       9.99172870e-01, 5.91397849e-01, 9.38792390e-01, 7.14640199e-01,\n",
      "       1.78660050e-01, 7.11331679e-01, 1.12489661e-01, 1.22415219e-01,\n",
      "       4.41149711e+01, 6.34470554e+03, 3.98004963e+02, 2.37965261e+00,\n",
      "       5.26972705e+01, 9.96691481e-01])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "train_df= df.selectExpr(\"features as features\")\n",
    "kmeans = KMeans(k=3, featuresCol='features')\n",
    "kmeans_model = kmeans.fit(train_df)\n",
    "predictions = kmeans_model.transform(train_df)\n",
    "evaluator = ClusteringEvaluator()\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette measure using squared Euclidean distance = \"+ str(silhouette\n",
    "                                                                    \n",
    "print(\"--*--\"*20)\n",
    "                                                                    \n",
    "cluster_centers = kmeans_model.clusterCenters()\n",
    "print(cluster_centers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fd5536",
   "metadata": {},
   "source": [
    "# Hierarchical clustering using bisecting K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54ecb589",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 222:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette measure using squared euclidean distance = 0.7914265424459597\n",
      "--*----*----*----*----*----*----*----*----*----*----*----*----*----*----*----*----*----*----*----*--\n",
      "[array([2.17575619e-01, 1.84922090e-01, 1.62923923e-01, 1.27520623e-01,\n",
      "       8.97112741e-02, 5.76306141e-02, 3.47158570e-02, 3.23098075e-02,\n",
      "       3.33409716e-02, 2.92163153e-02, 2.45187901e-02, 5.59692942e-01,\n",
      "       3.23556370e-01, 5.14550871e-01, 3.11984418e-01, 1.32218148e-01,\n",
      "       9.80980752e-01, 5.03208066e-01, 8.51397800e-01, 7.15857012e-01,\n",
      "       2.23189734e-01, 7.58936755e-01, 1.06324473e-01, 8.86801100e-02,\n",
      "       4.03988313e+01, 4.81029904e+02, 3.66856439e+02, 2.54812099e+00,\n",
      "       5.02430110e+01, 7.79903758e-01]), array([2.63910158e-01, 1.43440531e-01, 1.58754467e-01, 9.44359367e-02,\n",
      "       6.12557427e-02, 1.14854518e-01, 4.23685554e-02, 3.21592649e-02,\n",
      "       2.50127616e-02, 3.01174068e-02, 2.55232261e-02, 6.02348137e-01,\n",
      "       2.80245023e-01, 4.17049515e-01, 3.82337928e-01, 1.42930066e-01,\n",
      "       9.98979071e-01, 5.99285350e-01, 9.28024502e-01, 7.37621235e-01,\n",
      "       1.64369576e-01, 6.98825932e-01, 1.27616131e-01, 1.19448698e-01,\n",
      "       4.41807044e+01, 3.53988412e+03, 3.91451251e+02, 2.30525778e+00,\n",
      "       5.53379275e+01, 1.02603369e+00]), array([3.14465409e-01, 1.02725367e-01, 1.90775681e-01, 7.54716981e-02,\n",
      "       4.19287212e-02, 1.06918239e-01, 3.98322851e-02, 3.14465409e-02,\n",
      "       3.56394130e-02, 2.93501048e-02, 2.09643606e-02, 5.99580713e-01,\n",
      "       3.06079665e-01, 3.52201258e-01, 4.57023061e-01, 1.40461216e-01,\n",
      "       1.00000000e+00, 6.64570231e-01, 9.53878407e-01, 7.33752621e-01,\n",
      "       1.59329140e-01, 7.02306080e-01, 1.04821803e-01, 1.32075472e-01,\n",
      "       4.44423480e+01, 1.24569748e+04, 3.86513627e+02, 2.61006289e+00,\n",
      "       5.45492662e+01, 9.97903564e-01])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "train_df = df.selectExpr(\"features as features\")\n",
    "                                      \n",
    "bkmeans = BisectingKMeans(k=3, featuresCol='features')\n",
    "bkmeans_model = bkmeans.fit(train_df)\n",
    "predictions = bkmeans_model.transform(train_df)\n",
    "evaluator = ClusteringEvaluator()\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette measure using squared euclidean distance = \"+ str(silhouette))\n",
    "\n",
    "print(\"--*--\"*20)\n",
    "\n",
    "cluster_centers = bkmeans_model.clusterCenters()\n",
    "print(cluster_centers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54d805b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
