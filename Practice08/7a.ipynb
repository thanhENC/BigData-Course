{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "004b9809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/16 20:40:23 WARN Utils: Your hostname, m0 resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "22/10/16 20:40:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/16 20:40:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/10/16 20:40:31 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/10/16 20:40:31 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/10/16 20:40:31 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "22/10/16 20:40:31 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "22/10/16 20:40:31 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "22/10/16 20:40:31 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "22/10/16 20:40:31 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.\n",
      "22/10/16 20:40:31 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.\n",
      "22/10/16 20:40:31 WARN Utils: Service 'SparkUI' could not bind on port 4048. Attempting port 4049.\n",
      "22/10/16 20:40:31 WARN Utils: Service 'SparkUI' could not bind on port 4049. Attempting port 4050.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: string (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: string (nullable = true)\n",
      " |-- CustomerID: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "[('InvoiceNo', 'string'), ('StockCode', 'string'), ('Description', 'string'), ('Quantity', 'string'), ('InvoiceDate', 'string'), ('UnitPrice', 'string'), ('CustomerID', 'string'), ('Country', 'string')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "541909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of records: 541909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"7a\").getOrCreate()\n",
    "file_location = \"online_retail.csv\"\n",
    "file_type = \"csv\"\n",
    "infer_schema = \"False\"\n",
    "first_row_is_header = \"True\"\n",
    "df = spark.read.format(file_type) \\\n",
    ".option(\"inferSchema\", infer_schema) \\\n",
    ".option(\"header\", first_row_is_header) \\\n",
    ".load(file_location)\n",
    "df.printSchema()\n",
    "print(df.dtypes)\n",
    "print(df.count())\n",
    "print('The total number of records: '+str(df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99ee1e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|01/12/10 08:26|     4.25|     17850|United Kingdom|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|01/12/10 08:26|     7.65|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|01/12/10 08:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|01/12/10 08:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|01/12/10 08:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|01/12/10 08:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|01/12/10 08:26|     2.55|     17850|United Kingdom|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|01/12/10 08:28|     1.85|     17850|United Kingdom|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|01/12/10 08:28|     1.85|     17850|United Kingdom|\n",
      "|   536367|    21754|HOME BUILDING BLO...|       3|01/12/10 08:34|     5.95|     13047|United Kingdom|\n",
      "|   536367|    21755|LOVE BUILDING BLO...|       3|01/12/10 08:34|     5.95|     13047|United Kingdom|\n",
      "|   536367|    21777|RECIPE BOX WITH M...|       4|01/12/10 08:34|     7.95|     13047|United Kingdom|\n",
      "|   536367|    22310|IVORY KNITTED MUG...|       6|01/12/10 08:34|     1.65|     13047|United Kingdom|\n",
      "|   536367|    22622|BOX OF VINTAGE AL...|       2|01/12/10 08:34|     9.95|     13047|United Kingdom|\n",
      "|   536367|    22623|BOX OF VINTAGE JI...|       3|01/12/10 08:34|     4.95|     13047|United Kingdom|\n",
      "|   536367|    22745|POPPY'S PLAYHOUSE...|       6|01/12/10 08:34|      2.1|     13047|United Kingdom|\n",
      "|   536367|    22748|POPPY'S PLAYHOUSE...|       6|01/12/10 08:34|      2.1|     13047|United Kingdom|\n",
      "|   536367|    22749|FELTCRAFT PRINCES...|       8|01/12/10 08:34|     3.75|     13047|United Kingdom|\n",
      "|   536367|    48187| DOORMAT NEW ENGLAND|       4|01/12/10 08:34|     7.95|     13047|United Kingdom|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|01/12/10 08:34|     1.69|     13047|United Kingdom|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of records (dropped duplicates): 531232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dedupe_data = df.drop_duplicates([\"InvoiceNo\", \"InvoiceDate\", \"StockCode\"])\n",
    "dedupe_data.show()\n",
    "print('The total number of records (dropped duplicates): '+str(dedupe_data.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb025071",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp, col\n",
    "from pyspark.sql.types import FloatType\n",
    "interim_data = (select_data\n",
    "                  .withColumn(\"invoice_time\", to_timestamp(\"InvoiceDate|\", 'dd/M/yy HH:mm'))\n",
    "                  .withColumn(\"cust_age\", col(\"age\").cast(FloatType()))\n",
    "                  .withColumn(\"working_class\", col(\"work_class\").cast(FloatType()))\n",
    "                  .withColumn(\"fin_wt\", col(\"final_weight\").cast(FloatType()))\n",
    "                  .drop(\"final_weight\")\n",
    "             )\n",
    "interim_data.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7020097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md Data Loading\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %fs ls dbfs:/FileStore/shared_uploads/delta/retail_silver.delta\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "raw_data = spark.read.format(\"delta\").load(\"dbfs:/FileStore/shared_uploads/delta/retail_silver.delta\")\n",
    "raw_data.where(\"gender is not null\").display()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md Data Selection\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "select_data = (raw_data\n",
    "                .where(\"age is not null\")\n",
    "                .select(\"invoice_num\", \"stock_code\",\"quantity\", \"invoice_date\", \n",
    "                              \"unit_price\",\"country_code\",\"age\", \"gender\", \"occupation\", \"work_class\", \"final_weight\", \"description\"))\n",
    "select_data.display()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md Calculating basic Statistics\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "select_data.describe().display()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md Data Cleansing\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "dedupe_data = select_data.drop_duplicates([\"invoice_num\", \"invoice_date\", \"stock_code\"])\n",
    "dedupe_data.display()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md Change data types\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import to_timestamp, col\n",
    "from pyspark.sql.types import FloatType\n",
    "interim_data = (select_data\n",
    "                  .withColumn(\"invoice_time\", to_timestamp(\"invoice_date\", 'dd/M/yy HH:mm'))\n",
    "                  .withColumn(\"cust_age\", col(\"age\").cast(FloatType()))\n",
    "                  .withColumn(\"working_class\", col(\"work_class\").cast(FloatType()))\n",
    "                  .withColumn(\"fin_wt\", col(\"final_weight\").cast(FloatType()))\n",
    "                  .drop(\"final_weight\")\n",
    "             )\n",
    "interim_data.display()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md Replace NULL and NA values\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "clean_data = interim_data.na.fill(0)\n",
    "clean_data.display()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md Data Manipulation\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "final_data = (clean_data.where(\"age is not null\")\n",
    "                       .withColumnRenamed(\"working_class\", \"work_type\")\n",
    "                       .drop(\"age\")\n",
    "                       .drop(\"work_class\")\n",
    "                       .drop(\"fn_wt\")\n",
    "             )\n",
    "final_data.display()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "#%fs rm -r /FileStore/shared_uploads/delta/retail_ml.delta\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "(final_data.where(\"description is NOT NULL\")\n",
    "       .write.format(\"delta\").mode(\"overwrite\")\n",
    "         .save(\"dbfs:/FileStore/shared_uploads/delta/retail_ml.delta\"))\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "final_data.count()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md Convert Spark DataFrame to a Pandas Dataframe\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "pd_data = final_data.toPandas()\n",
    "pd_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
