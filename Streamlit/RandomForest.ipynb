{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Tham khao](https://towardsdatascience.com/a-guide-to-exploit-random-forest-classifier-in-pyspark-46d6999cb5db)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Driving_License: integer (nullable = true)\n",
      " |-- Region_Code: double (nullable = true)\n",
      " |-- Previously_Insured: integer (nullable = true)\n",
      " |-- Vehicle_Age: string (nullable = true)\n",
      " |-- Vehicle_Damage: string (nullable = true)\n",
      " |-- Annual_Premium: double (nullable = true)\n",
      " |-- Policy_Sales_Channel: double (nullable = true)\n",
      " |-- Vintage: integer (nullable = true)\n",
      " |-- Response: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('ML-RandomForest').getOrCreate()\n",
    "df = spark.read.csv('Dataset/train.csv', header = True, inferSchema = True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+---------------+-----------+------------------+-----------+--------------+--------------+--------------------+-------+--------+\n",
      "| id|Gender|Age|Driving_License|Region_Code|Previously_Insured|Vehicle_Age|Vehicle_Damage|Annual_Premium|Policy_Sales_Channel|Vintage|Response|\n",
      "+---+------+---+---------------+-----------+------------------+-----------+--------------+--------------+--------------------+-------+--------+\n",
      "|  1|  Male| 44|              1|       28.0|                 0|  > 2 Years|           Yes|       40454.0|                26.0|    217|       1|\n",
      "|  2|  Male| 76|              1|        3.0|                 0|   1-2 Year|            No|       33536.0|                26.0|    183|       0|\n",
      "|  3|  Male| 47|              1|       28.0|                 0|  > 2 Years|           Yes|       38294.0|                26.0|     27|       1|\n",
      "|  4|  Male| 21|              1|       11.0|                 1|   < 1 Year|            No|       28619.0|               152.0|    203|       0|\n",
      "|  5|Female| 29|              1|       41.0|                 1|   < 1 Year|            No|       27496.0|               152.0|     39|       0|\n",
      "|  6|Female| 24|              1|       33.0|                 0|   < 1 Year|           Yes|        2630.0|               160.0|    176|       0|\n",
      "|  7|  Male| 23|              1|       11.0|                 0|   < 1 Year|           Yes|       23367.0|               152.0|    249|       0|\n",
      "|  8|Female| 56|              1|       28.0|                 0|   1-2 Year|           Yes|       32031.0|                26.0|     72|       1|\n",
      "|  9|Female| 24|              1|        3.0|                 1|   < 1 Year|            No|       27619.0|               152.0|     28|       0|\n",
      "| 10|Female| 32|              1|        6.0|                 1|   < 1 Year|            No|       28771.0|               152.0|     80|       0|\n",
      "+---+------+---+---------------+-----------+------------------+-----------+--------------+--------------+--------------------+-------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count distinct value of all columns\n",
    "from pyspark.sql.functions import countDistinct\n",
    "df.select([countDistinct(c).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dont run this section - Draft Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = ['Age', 'Region_Code', 'Annual_Premium', 'Vintage']\n",
    "categorical_columns = ['Gender', 'Driving_License', 'Previously_Insured', 'Vehicle_Age', 'Vehicle_Damage', 'Response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+---------------+-----------+------------------+-----------+--------------+--------------+--------------------+-------+--------+--------------------+--------------------+\n",
      "| id|Gender|Age|Driving_License|Region_Code|Previously_Insured|Vehicle_Age|Vehicle_Damage|Annual_Premium|Policy_Sales_Channel|Vintage|Response|            features|      scaledFeatures|\n",
      "+---+------+---+---------------+-----------+------------------+-----------+--------------+--------------+--------------------+-------+--------+--------------------+--------------------+\n",
      "|  1|  Male| 44|              1|       28.0|                 0|  > 2 Years|           Yes|       40454.0|                26.0|    217|       1|[44.0,28.0,40454....|[0.36923076923076...|\n",
      "|  2|  Male| 76|              1|        3.0|                 0|   1-2 Year|            No|       33536.0|                26.0|    183|       0|[76.0,3.0,33536.0...|[0.86153846153846...|\n",
      "|  3|  Male| 47|              1|       28.0|                 0|  > 2 Years|           Yes|       38294.0|                26.0|     27|       1|[47.0,28.0,38294....|[0.41538461538461...|\n",
      "|  4|  Male| 21|              1|       11.0|                 1|   < 1 Year|            No|       28619.0|               152.0|    203|       0|[21.0,11.0,28619....|[0.01538461538461...|\n",
      "|  5|Female| 29|              1|       41.0|                 1|   < 1 Year|            No|       27496.0|               152.0|     39|       0|[29.0,41.0,27496....|[0.13846153846153...|\n",
      "|  6|Female| 24|              1|       33.0|                 0|   < 1 Year|           Yes|        2630.0|               160.0|    176|       0|[24.0,33.0,2630.0...|[0.06153846153846...|\n",
      "|  7|  Male| 23|              1|       11.0|                 0|   < 1 Year|           Yes|       23367.0|               152.0|    249|       0|[23.0,11.0,23367....|[0.04615384615384...|\n",
      "|  8|Female| 56|              1|       28.0|                 0|   1-2 Year|           Yes|       32031.0|                26.0|     72|       1|[56.0,28.0,32031....|[0.55384615384615...|\n",
      "|  9|Female| 24|              1|        3.0|                 1|   < 1 Year|            No|       27619.0|               152.0|     28|       0|[24.0,3.0,27619.0...|[0.06153846153846...|\n",
      "| 10|Female| 32|              1|        6.0|                 1|   < 1 Year|            No|       28771.0|               152.0|     80|       0|[32.0,6.0,28771.0...|[0.18461538461538...|\n",
      "+---+------+---+---------------+-----------+------------------+-----------+--------------+--------------+--------------------+-------+--------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#MinMaxScaler on numerical columns\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols = numerical_columns, outputCol = 'features')\n",
    "output = assembler.transform(df)\n",
    "\n",
    "scaler = MinMaxScaler(inputCol = 'features', outputCol = 'scaledFeatures')\n",
    "scalerModel = scaler.fit(output)\n",
    "scaledData = scalerModel.transform(output)\n",
    "\n",
    "scaledData.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into train and test\n",
    "train, test = scaledData.randomSplit([0.7, 0.3], seed = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'Response')\n",
    "rfModel = rf.fit(train)\n",
    "predictions = rfModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+---------------+-----------+------------------+-----------+--------------+--------------+--------------------+-------+--------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "| id|Gender|Age|Driving_License|Region_Code|Previously_Insured|Vehicle_Age|Vehicle_Damage|Annual_Premium|Policy_Sales_Channel|Vintage|Response|            features|      scaledFeatures|       rawPrediction|         probability|prediction|\n",
      "+---+------+---+---------------+-----------+------------------+-----------+--------------+--------------+--------------------+-------+--------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|  3|  Male| 47|              1|       28.0|                 0|  > 2 Years|           Yes|       38294.0|                26.0|     27|       1|[47.0,28.0,38294....|[0.41538461538461...|[17.4351914968468...|[0.87175957484234...|       0.0|\n",
      "|  7|  Male| 23|              1|       11.0|                 0|   < 1 Year|           Yes|       23367.0|               152.0|    249|       0|[23.0,11.0,23367....|[0.04615384615384...|[17.7182393925864...|[0.88591196962932...|       0.0|\n",
      "| 10|Female| 32|              1|        6.0|                 1|   < 1 Year|            No|       28771.0|               152.0|     80|       0|[32.0,6.0,28771.0...|[0.18461538461538...|[17.4351914968468...|[0.87175957484234...|       0.0|\n",
      "| 13|Female| 41|              1|       15.0|                 1|   1-2 Year|            No|       31409.0|                14.0|    221|       0|[41.0,15.0,31409....|[0.32307692307692...|[17.4351914968468...|[0.87175957484234...|       0.0|\n",
      "| 14|  Male| 76|              1|       28.0|                 0|   1-2 Year|           Yes|       36770.0|                13.0|     15|       0|[76.0,28.0,36770....|[0.86153846153846...|[17.4351914968468...|[0.87175957484234...|       0.0|\n",
      "| 19|  Male| 42|              1|       28.0|                 0|   1-2 Year|           Yes|       33667.0|               124.0|    158|       0|[42.0,28.0,33667....|[0.33846153846153...|[17.4351914968468...|[0.87175957484234...|       0.0|\n",
      "| 24|  Male| 44|              1|       28.0|                 0|   1-2 Year|           Yes|       41852.0|               163.0|     60|       0|[44.0,28.0,41852....|[0.36923076923076...|[17.4351914968468...|[0.87175957484234...|       0.0|\n",
      "| 30|  Male| 57|              1|       11.0|                 0|   1-2 Year|           Yes|       25679.0|               157.0|    232|       1|[57.0,11.0,25679....|[0.56923076923076...|[17.4351914968468...|[0.87175957484234...|       0.0|\n",
      "| 31|Female| 26|              1|        8.0|                 0|   < 1 Year|            No|        2630.0|               160.0|    136|       0|[26.0,8.0,2630.0,...|[0.09230769230769...|[17.6885904561183...|[0.88442952280591...|       0.0|\n",
      "| 34|Female| 24|              1|       36.0|                 0|   < 1 Year|           Yes|       43129.0|               152.0|     49|       0|[24.0,36.0,43129....|[0.06153846153846...|[17.7182393925864...|[0.88591196962932...|       0.0|\n",
      "| 35|Female| 32|              1|       30.0|                 1|   < 1 Year|            No|       27638.0|               152.0|    169|       0|[32.0,30.0,27638....|[0.18461538461538...|[17.4351914968468...|[0.87175957484234...|       0.0|\n",
      "| 36|  Male| 41|              1|       36.0|                 1|   1-2 Year|            No|       30039.0|               124.0|     88|       0|[41.0,36.0,30039....|[0.32307692307692...|[17.4351914968468...|[0.87175957484234...|       0.0|\n",
      "| 39|  Male| 45|              1|        8.0|                 0|   1-2 Year|           Yes|       42297.0|               124.0|    264|       0|[45.0,8.0,42297.0...|[0.38461538461538...|[17.4351914968468...|[0.87175957484234...|       0.0|\n",
      "| 41|  Male| 30|              1|       30.0|                 0|   < 1 Year|           Yes|       24550.0|               124.0|     45|       0|[30.0,30.0,24550....|[0.15384615384615...|[17.4351914968468...|[0.87175957484234...|       0.0|\n",
      "| 47|Female| 29|              1|       15.0|                 1|   < 1 Year|            No|       32923.0|               152.0|     34|       0|[29.0,15.0,32923....|[0.13846153846153...|[17.7182393925864...|[0.88591196962932...|       0.0|\n",
      "| 48|  Male| 51|              1|       26.0|                 1|   1-2 Year|            No|       33761.0|                22.0|     83|       0|[51.0,26.0,33761....|[0.47692307692307...|[17.4351914968468...|[0.87175957484234...|       0.0|\n",
      "| 56|Female| 20|              1|       47.0|                 0|   < 1 Year|           Yes|       25380.0|               160.0|    171|       0|[20.0,47.0,25380....|[0.0,0.9038461538...|[17.7182393925864...|[0.88591196962932...|       0.0|\n",
      "| 59|Female| 62|              1|       48.0|                 0|   1-2 Year|           Yes|        2630.0|                15.0|    295|       0|[62.0,48.0,2630.0...|[0.64615384615384...|[17.5662481856710...|[0.87831240928355...|       0.0|\n",
      "| 61|  Male| 49|              1|        8.0|                 0|   1-2 Year|           Yes|        2630.0|               156.0|     30|       0|[49.0,8.0,2630.0,...|[0.44615384615384...|[17.4351914968468...|[0.87175957484234...|       0.0|\n",
      "| 62|Female| 20|              1|       50.0|                 0|   < 1 Year|           Yes|       36615.0|               160.0|    218|       0|[20.0,50.0,36615....|[0.0,0.9615384615...|[17.7182393925864...|[0.88591196962932...|       0.0|\n",
      "| 65|  Male| 39|              1|       15.0|                 0|   1-2 Year|           Yes|       37849.0|               124.0|     22|       0|[39.0,15.0,37849....|[0.29230769230769...|[17.4351914968468...|[0.87175957484234...|       0.0|\n",
      "| 68|  Male| 60|              1|       28.0|                 0|   1-2 Year|           Yes|       66338.0|               124.0|     73|       0|[60.0,28.0,66338....|[0.61538461538461...|[17.4351914968468...|[0.87175957484234...|       0.0|\n",
      "| 74|  Male| 50|              1|       28.0|                 1|   1-2 Year|            No|       27784.0|                26.0|    277|       0|[50.0,28.0,27784....|[0.46153846153846...|[17.4351914968468...|[0.87175957484234...|       0.0|\n",
      "| 78|Female| 27|              1|       33.0|                 1|   < 1 Year|            No|       35512.0|               152.0|    111|       0|[27.0,33.0,35512....|[0.10769230769230...|[17.7182393925864...|[0.88591196962932...|       0.0|\n",
      "| 80|  Male| 77|              1|       37.0|                 1|   1-2 Year|            No|       40651.0|               124.0|    183|       0|[77.0,37.0,40651....|[0.87692307692307...|[17.4351914968468...|[0.87175957484234...|       0.0|\n",
      "+---+------+---+---------------+-----------+------------------+-----------+--------------+--------------+--------------------+-------+--------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|Response|prediction|\n",
      "+--------+----------+\n",
      "|       1|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       1|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "+--------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"Response\", \"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.8198065256599442\n",
      "Test Error = 0.18019347434005584\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Response\", predictionCol=\"prediction\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = %s\" % (accuracy))\n",
    "print(\"Test Error = %s\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pyspark\\sql\\context.py:157: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[99950.     0.]\n",
      " [13993.     0.]]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.types import FloatType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "preds_and_labels = predictions.select(['prediction','Response']).withColumn('Response', F.col('Response').cast(FloatType())).orderBy('prediction')\n",
    "preds_and_labels = preds_and_labels.select(['prediction','Response'])\n",
    "metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
    "print(metrics.confusionMatrix().toArray())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. New Section - Run this section"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Split Vehicle_Age into 3 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical_columns = ['Age', 'Region_Code', 'Annual_Premium', 'Vintage']\n",
    "# categorical_columns = ['Gender', 'Driving_License', 'Previously_Insured', 'Vehicle_Age', 'Vehicle_Damage', 'Response']\n",
    "input_cols = ['Age', 'Vehicle_Age_0', 'Vehicle_Age_1', 'Vehicle_Age_2', 'Previously_Insured', 'Vehicle_Damage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert Vehicle_Age to numerical\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "df = df.withColumn('Vehicle_Age_0', when(df['Vehicle_Age'] == '< 1 Year', 0.0).when(df['Vehicle_Age'] == '1-2 Year', 1.0).otherwise(2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+---------------+-----------+------------------+--------------+--------------+--------------------+-------+--------+-------------+-------------+-------------+\n",
      "| id|Gender|Age|Driving_License|Region_Code|Previously_Insured|Vehicle_Damage|Annual_Premium|Policy_Sales_Channel|Vintage|Response|Vehicle_Age_0|Vehicle_Age_1|Vehicle_Age_2|\n",
      "+---+------+---+---------------+-----------+------------------+--------------+--------------+--------------------+-------+--------+-------------+-------------+-------------+\n",
      "|  1|  Male| 44|              1|       28.0|                 0|             1|       40454.0|                26.0|    217|       1|            0|            0|            1|\n",
      "|  2|  Male| 76|              1|        3.0|                 0|             0|       33536.0|                26.0|    183|       0|            0|            1|            0|\n",
      "|  3|  Male| 47|              1|       28.0|                 0|             1|       38294.0|                26.0|     27|       1|            0|            0|            1|\n",
      "|  4|  Male| 21|              1|       11.0|                 1|             0|       28619.0|               152.0|    203|       0|            1|            0|            0|\n",
      "|  5|Female| 29|              1|       41.0|                 1|             0|       27496.0|               152.0|     39|       0|            1|            0|            0|\n",
      "+---+------+---+---------------+-----------+------------------+--------------+--------------+--------------------+-------+--------+-------------+-------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "#new column Vehicle_Age_0 for Vehicle_Age < 1 Year\n",
    "df = df.withColumn('Vehicle_Age_0', F.when(F.col('Vehicle_Age') == '< 1 Year', 1).otherwise(0))\n",
    "#new column Vehicle_Age_1 for Vehicle_Age 1-2 Year\n",
    "df = df.withColumn('Vehicle_Age_1', F.when(F.col('Vehicle_Age') == '1-2 Year', 1).otherwise(0))\n",
    "#new column Vehicle_Age_2 for Vehicle_Age > 2 Years\n",
    "df = df.withColumn('Vehicle_Age_2', F.when(F.col('Vehicle_Age') == '> 2 Years', 1).otherwise(0))\n",
    "#drop Vehicle_Age column\n",
    "df = df.drop('Vehicle_Age')\n",
    "#Vehicle_Damage column to 0 and 1\n",
    "df = df.withColumn('Vehicle_Damage', F.when(F.col('Vehicle_Damage') == 'Yes', 1).otherwise(0))\n",
    "df.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Resampling Imbalanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Response', ylabel='count'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#EDA\n",
    "import pandas as pd\n",
    "df_EDA = df.toPandas()\n",
    "#countplot for df_train['Response']\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "sns.countplot(x='Response', data=df_EDA[['Response']])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Way 1 or Way 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Way 1: Decrease the number of samples in the majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_56060\\4125765123.py:3: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_EDA_balance = df_EDA_balance.append(df_EDA[df_EDA['Response'] == 0].sample(n=46710, random_state=123))\n"
     ]
    }
   ],
   "source": [
    "#data is imbalance, response = 0 is much more than response = 1, remove some rows of response = 0\n",
    "df_EDA_balance = df_EDA[df_EDA['Response'] == 1]\n",
    "df_EDA_balance = df_EDA_balance.append(df_EDA[df_EDA['Response'] == 0].sample(n=46710, random_state=123))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Response', ylabel='count'>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x='Response', data=df_EDA_balance[['Response']])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Way 2: Increase the number of samples in the minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Response', ylabel='count'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#data is imbalanced, so we need to balance it, resample dataset\n",
    "from sklearn.utils import resample\n",
    "df_majority = df_EDA[df_EDA['Response'] == 0]\n",
    "df_minority = df_EDA[df_EDA['Response'] == 1]\n",
    "df_minority_upsampled = resample(df_minority, replace=True, n_samples=334399, random_state=123)\n",
    "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    "df_upsampled['Response'].value_counts()\n",
    "\n",
    "sns.countplot(x='Response', data=df_upsampled[['Response']])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Vectorize - Skip Minmaxscaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+---------------+-----------+------------------+--------------+--------------+--------------------+-------+--------+-------------+-------------+-------------+--------------------+\n",
      "| id|Gender|Age|Driving_License|Region_Code|Previously_Insured|Vehicle_Damage|Annual_Premium|Policy_Sales_Channel|Vintage|Response|Vehicle_Age_0|Vehicle_Age_1|Vehicle_Age_2|            features|\n",
      "+---+------+---+---------------+-----------+------------------+--------------+--------------+--------------------+-------+--------+-------------+-------------+-------------+--------------------+\n",
      "|  2|  Male| 76|              1|        3.0|                 0|             0|       33536.0|                26.0|    183|       0|            0|            1|            0|(6,[0,2],[76.0,1.0])|\n",
      "|  4|  Male| 21|              1|       11.0|                 1|             0|       28619.0|               152.0|    203|       0|            1|            0|            0|[21.0,1.0,0.0,0.0...|\n",
      "|  5|Female| 29|              1|       41.0|                 1|             0|       27496.0|               152.0|     39|       0|            1|            0|            0|[29.0,1.0,0.0,0.0...|\n",
      "|  6|Female| 24|              1|       33.0|                 0|             1|        2630.0|               160.0|    176|       0|            1|            0|            0|[24.0,1.0,0.0,0.0...|\n",
      "|  7|  Male| 23|              1|       11.0|                 0|             1|       23367.0|               152.0|    249|       0|            1|            0|            0|[23.0,1.0,0.0,0.0...|\n",
      "|  9|Female| 24|              1|        3.0|                 1|             0|       27619.0|               152.0|     28|       0|            1|            0|            0|[24.0,1.0,0.0,0.0...|\n",
      "| 10|Female| 32|              1|        6.0|                 1|             0|       28771.0|               152.0|     80|       0|            1|            0|            0|[32.0,1.0,0.0,0.0...|\n",
      "| 12|Female| 24|              1|       50.0|                 1|             0|       48699.0|               152.0|    289|       0|            1|            0|            0|[24.0,1.0,0.0,0.0...|\n",
      "| 13|Female| 41|              1|       15.0|                 1|             0|       31409.0|                14.0|    221|       0|            0|            1|            0|[41.0,0.0,1.0,0.0...|\n",
      "| 14|  Male| 76|              1|       28.0|                 0|             1|       36770.0|                13.0|     15|       0|            0|            1|            0|[76.0,0.0,1.0,0.0...|\n",
      "+---+------+---+---------------+-----------+------------------+--------------+--------------+--------------------+-------+--------+-------------+-------------+-------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(df_upsampled)\n",
    "#MinMaxScaler on numerical columns\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols = input_cols, outputCol = 'features')\n",
    "output = assembler.transform(df)\n",
    "output.show(10)\n",
    "\n",
    "# scaler = MinMaxScaler(inputCol = 'features', outputCol = 'scaledFeatures')\n",
    "# scalerModel = scaler.fit(output)\n",
    "# scaledData = scalerModel.transform(output)\n",
    "\n",
    "# scaledData.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into train and test\n",
    "train, test = output.randomSplit([0.7, 0.3], seed = 123)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Training Model Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+---------------+-----------+------------------+--------------+--------------+--------------------+-------+--------+-------------+-------------+-------------+--------------------+--------------------+--------------------+----------+\n",
      "| id|Gender|Age|Driving_License|Region_Code|Previously_Insured|Vehicle_Damage|Annual_Premium|Policy_Sales_Channel|Vintage|Response|Vehicle_Age_0|Vehicle_Age_1|Vehicle_Age_2|            features|       rawPrediction|         probability|prediction|\n",
      "+---+------+---+---------------+-----------+------------------+--------------+--------------+--------------------+-------+--------+-------------+-------------+-------------+--------------------+--------------------+--------------------+----------+\n",
      "|  5|Female| 29|              1|       41.0|                 1|             0|       27496.0|               152.0|     39|       0|            1|            0|            0|[29.0,1.0,0.0,0.0...|[19.5333902086973...|[0.97666951043486...|       0.0|\n",
      "| 10|Female| 32|              1|        6.0|                 1|             0|       28771.0|               152.0|     80|       0|            1|            0|            0|[32.0,1.0,0.0,0.0...|[19.5333902086973...|[0.97666951043486...|       0.0|\n",
      "| 14|  Male| 76|              1|       28.0|                 0|             1|       36770.0|                13.0|     15|       0|            0|            1|            0|[76.0,0.0,1.0,0.0...|[6.85626683709140...|[0.34281334185457...|       1.0|\n",
      "| 18|Female| 25|              1|       35.0|                 1|             0|       46622.0|               152.0|    299|       0|            1|            0|            0|[25.0,1.0,0.0,0.0...|[19.5333902086973...|[0.97666951043486...|       0.0|\n",
      "| 19|  Male| 42|              1|       28.0|                 0|             1|       33667.0|               124.0|    158|       0|            0|            1|            0|[42.0,0.0,1.0,0.0...|[5.12804342523099...|[0.25640217126154...|       1.0|\n",
      "| 25|  Male| 34|              1|       15.0|                 1|             0|       38111.0|               152.0|    180|       0|            0|            1|            0|[34.0,0.0,1.0,0.0...|[19.4465377947431...|[0.97232688973715...|       0.0|\n",
      "| 32|  Male| 79|              1|       28.0|                 0|             1|       57444.0|                26.0|    222|       0|            0|            1|            0|[79.0,0.0,1.0,0.0...|[6.85626683709140...|[0.34281334185457...|       1.0|\n",
      "| 38|Female| 25|              1|       28.0|                 1|             0|       76251.0|               152.0|    107|       0|            1|            0|            0|[25.0,1.0,0.0,0.0...|[19.5333902086973...|[0.97666951043486...|       0.0|\n",
      "| 39|  Male| 45|              1|        8.0|                 0|             1|       42297.0|               124.0|    264|       0|            0|            1|            0|[45.0,0.0,1.0,0.0...|[5.12804342523099...|[0.25640217126154...|       1.0|\n",
      "| 42|Female| 54|              1|       28.0|                 0|             0|       38560.0|               122.0|    184|       0|            0|            1|            0|(6,[0,2],[54.0,1.0])|[17.5083226387162...|[0.87541613193581...|       0.0|\n",
      "| 43|  Male| 27|              1|       41.0|                 0|             0|       21078.0|               152.0|    251|       0|            1|            0|            0|(6,[0,1],[27.0,1.0])|[17.8980790353328...|[0.89490395176664...|       0.0|\n",
      "| 44|Female| 38|              1|       35.0|                 1|             0|        2630.0|               152.0|    153|       0|            0|            1|            0|[38.0,0.0,1.0,0.0...|[19.4465377947431...|[0.97232688973715...|       0.0|\n",
      "| 47|Female| 29|              1|       15.0|                 1|             0|       32923.0|               152.0|     34|       0|            1|            0|            0|[29.0,1.0,0.0,0.0...|[19.5333902086973...|[0.97666951043486...|       0.0|\n",
      "| 49|Female| 25|              1|       15.0|                 0|             0|       38667.0|               152.0|     12|       0|            1|            0|            0|(6,[0,1],[25.0,1.0])|[17.7429843598163...|[0.88714921799081...|       0.0|\n",
      "| 55|Female| 44|              1|       28.0|                 0|             1|       45415.0|                13.0|     73|       0|            0|            1|            0|[44.0,0.0,1.0,0.0...|[5.12804342523099...|[0.25640217126154...|       1.0|\n",
      "| 56|Female| 20|              1|       47.0|                 0|             1|       25380.0|               160.0|    171|       0|            1|            0|            0|[20.0,1.0,0.0,0.0...|[11.3633980851095...|[0.56816990425547...|       0.0|\n",
      "| 65|  Male| 39|              1|       15.0|                 0|             1|       37849.0|               124.0|     22|       0|            0|            1|            0|[39.0,0.0,1.0,0.0...|[5.12804342523099...|[0.25640217126154...|       1.0|\n",
      "| 68|  Male| 60|              1|       28.0|                 0|             1|       66338.0|               124.0|     73|       0|            0|            1|            0|[60.0,0.0,1.0,0.0...|[5.93475994835639...|[0.29673799741781...|       1.0|\n",
      "| 71|  Male| 44|              1|       47.0|                 0|             1|       46481.0|                16.0|    100|       0|            0|            1|            0|[44.0,0.0,1.0,0.0...|[5.12804342523099...|[0.25640217126154...|       1.0|\n",
      "| 72|Female| 27|              1|       11.0|                 0|             1|       34587.0|                26.0|     63|       0|            1|            0|            0|[27.0,1.0,0.0,0.0...|[8.21356706121281...|[0.41067835306064...|       1.0|\n",
      "| 75|  Male| 59|              1|       23.0|                 0|             0|       27865.0|               124.0|     39|       0|            0|            1|            0|(6,[0,2],[59.0,1.0])|[17.7160723454911...|[0.88580361727455...|       0.0|\n",
      "| 78|Female| 27|              1|       33.0|                 1|             0|       35512.0|               152.0|    111|       0|            1|            0|            0|[27.0,1.0,0.0,0.0...|[19.5333902086973...|[0.97666951043486...|       0.0|\n",
      "| 85|  Male| 69|              1|       28.0|                 0|             1|       29802.0|               122.0|    158|       0|            0|            1|            0|[69.0,0.0,1.0,0.0...|[6.75391072949404...|[0.33769553647470...|       1.0|\n",
      "| 89|Female| 23|              1|       35.0|                 1|             0|       27606.0|               160.0|    114|       0|            1|            0|            0|[23.0,1.0,0.0,0.0...|[19.5333902086973...|[0.97666951043486...|       0.0|\n",
      "| 92|Female| 26|              1|       30.0|                 1|             0|       31791.0|               152.0|    189|       0|            1|            0|            0|[26.0,1.0,0.0,0.0...|[19.5333902086973...|[0.97666951043486...|       0.0|\n",
      "+---+------+---+---------------+-----------+------------------+--------------+--------------+--------------------+-------+--------+-------------+-------------+-------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 25 rows\n",
      "\n",
      "+--------+----------+\n",
      "|Response|prediction|\n",
      "+--------+----------+\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       1.0|\n",
      "|       0|       0.0|\n",
      "|       0|       1.0|\n",
      "|       0|       0.0|\n",
      "|       0|       1.0|\n",
      "|       0|       0.0|\n",
      "|       0|       1.0|\n",
      "|       0|       0.0|\n",
      "+--------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'Response')\n",
    "rfModel = rf.fit(train)\n",
    "predictions = rfModel.transform(test)\n",
    "\n",
    "predictions.show(25)\n",
    "\n",
    "predictions.select(\"Response\", \"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.7830395039637373\n",
      "Test Error = 0.21696049603626266\n"
     ]
    }
   ],
   "source": [
    "#Validation\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Response\", predictionCol=\"prediction\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = %s\" % (accuracy))\n",
    "print(\"Test Error = %s\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\lib\\site-packages\\pyspark\\sql\\context.py:157: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[61334. 38564.]\n",
      " [ 3403. 96287.]]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.types import FloatType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "preds_and_labels = predictions.select(['prediction','Response']).withColumn('Response', F.col('Response').cast(FloatType())).orderBy('prediction')\n",
    "preds_and_labels = preds_and_labels.select(['prediction','Response'])\n",
    "metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
    "print(metrics.confusionMatrix().toArray())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Export Model - DONT RUN IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o356.save.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.hadoop.mapred.FileOutputCommitter.setupJob(FileOutputCommitter.java:131)\r\n\tat org.apache.hadoop.mapred.OutputCommitter.setupJob(OutputCommitter.java:265)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:79)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1599)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1599)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1585)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1585)\r\n\tat org.apache.spark.ml.util.DefaultParamsWriter$.saveMetadata(ReadWrite.scala:413)\r\n\tat org.apache.spark.ml.util.DefaultParamsWriter.saveImpl(ReadWrite.scala:384)\r\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:168)\r\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:343)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:344)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:901)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 22 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mf:\\OneDrive - itbclub\\Hoc Ky\\Hoc Ky 05\\02. Big Data\\01. Practice\\BigData-Course\\Streamlit\\RandomForest.ipynb Cell 34\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/OneDrive%20-%20itbclub/Hoc%20Ky/Hoc%20Ky%2005/02.%20Big%20Data/01.%20Practice/BigData-Course/Streamlit/RandomForest.ipynb#X50sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m rf\u001b[39m.\u001b[39;49msave(\u001b[39m'\u001b[39;49m\u001b[39mrf_model_K20411\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\lib\\site-packages\\pyspark\\ml\\util.py:246\u001b[0m, in \u001b[0;36mMLWritable.save\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msave\u001b[39m(\u001b[39mself\u001b[39m, path: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    245\u001b[0m     \u001b[39m\"\"\"Save this ML instance to the given path, a shortcut of 'write().save(path)'.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 246\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwrite()\u001b[39m.\u001b[39;49msave(path)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\lib\\site-packages\\pyspark\\ml\\util.py:197\u001b[0m, in \u001b[0;36mJavaMLWriter.save\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(path, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    196\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mpath should be a string, got type \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(path))\n\u001b[1;32m--> 197\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49msave(path)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    191\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o356.save.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.hadoop.mapred.FileOutputCommitter.setupJob(FileOutputCommitter.java:131)\r\n\tat org.apache.hadoop.mapred.OutputCommitter.setupJob(OutputCommitter.java:265)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:79)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1599)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1599)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1585)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1585)\r\n\tat org.apache.spark.ml.util.DefaultParamsWriter$.saveMetadata(ReadWrite.scala:413)\r\n\tat org.apache.spark.ml.util.DefaultParamsWriter.saveImpl(ReadWrite.scala:384)\r\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:168)\r\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:343)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:344)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:901)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 22 more\r\n"
     ]
    }
   ],
   "source": [
    "rf.save('rf_model_K20411')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1091.save.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.hadoop.mapred.FileOutputCommitter.setupJob(FileOutputCommitter.java:131)\r\n\tat org.apache.hadoop.mapred.OutputCommitter.setupJob(OutputCommitter.java:265)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:79)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1599)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1599)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1585)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1585)\r\n\tat org.apache.spark.ml.util.DefaultParamsWriter$.saveMetadata(ReadWrite.scala:413)\r\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$saveImpl$1(Pipeline.scala:250)\r\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$saveImpl$1$adapted(Pipeline.scala:247)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.saveImpl(Pipeline.scala:247)\r\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.saveImpl(Pipeline.scala:346)\r\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:168)\r\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.super$save(Pipeline.scala:344)\r\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$4(Pipeline.scala:344)\r\n\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent(events.scala:174)\r\n\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent$(events.scala:169)\r\n\tat org.apache.spark.ml.util.Instrumentation.withSaveInstanceEvent(Instrumentation.scala:42)\r\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3(Pipeline.scala:344)\r\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3$adapted(Pipeline.scala:344)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.save(Pipeline.scala:344)\r\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:343)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:344)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:901)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 22 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mf:\\OneDrive - itbclub\\Hoc Ky\\Hoc Ky 05\\02. Big Data\\01. Practice\\BigData-Course\\Streamlit\\RandomForest.ipynb Cell 32\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/OneDrive%20-%20itbclub/Hoc%20Ky/Hoc%20Ky%2005/02.%20Big%20Data/01.%20Practice/BigData-Course/Streamlit/RandomForest.ipynb#X34sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m pipeline \u001b[39m=\u001b[39m Pipeline(stages\u001b[39m=\u001b[39m[assembler, rf])\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/OneDrive%20-%20itbclub/Hoc%20Ky/Hoc%20Ky%2005/02.%20Big%20Data/01.%20Practice/BigData-Course/Streamlit/RandomForest.ipynb#X34sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m pipelineModel \u001b[39m=\u001b[39m pipeline\u001b[39m.\u001b[39mfit(df)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/OneDrive%20-%20itbclub/Hoc%20Ky/Hoc%20Ky%2005/02.%20Big%20Data/01.%20Practice/BigData-Course/Streamlit/RandomForest.ipynb#X34sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m pipelineModel\u001b[39m.\u001b[39;49mwrite()\u001b[39m.\u001b[39;49moverwrite()\u001b[39m.\u001b[39;49msave(\u001b[39m'\u001b[39;49m\u001b[39mrf_model\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\lib\\site-packages\\pyspark\\ml\\util.py:197\u001b[0m, in \u001b[0;36mJavaMLWriter.save\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(path, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    196\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mpath should be a string, got type \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(path))\n\u001b[1;32m--> 197\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49msave(path)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    191\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1091.save.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.hadoop.mapred.FileOutputCommitter.setupJob(FileOutputCommitter.java:131)\r\n\tat org.apache.hadoop.mapred.OutputCommitter.setupJob(OutputCommitter.java:265)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:79)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1599)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1599)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1585)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1585)\r\n\tat org.apache.spark.ml.util.DefaultParamsWriter$.saveMetadata(ReadWrite.scala:413)\r\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$saveImpl$1(Pipeline.scala:250)\r\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$saveImpl$1$adapted(Pipeline.scala:247)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.saveImpl(Pipeline.scala:247)\r\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.saveImpl(Pipeline.scala:346)\r\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:168)\r\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.super$save(Pipeline.scala:344)\r\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$4(Pipeline.scala:344)\r\n\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent(events.scala:174)\r\n\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent$(events.scala:169)\r\n\tat org.apache.spark.ml.util.Instrumentation.withSaveInstanceEvent(Instrumentation.scala:42)\r\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3(Pipeline.scala:344)\r\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3$adapted(Pipeline.scala:344)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.save(Pipeline.scala:344)\r\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:343)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:344)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:901)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 22 more\r\n"
     ]
    }
   ],
   "source": [
    "#Export model\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[assembler, rf])\n",
    "pipelineModel = pipeline.fit(df)\n",
    "pipelineModel.write().overwrite().save('rf_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamlit_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:39:17) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2db37e07425b399dc4d0df31cc6122067e595125cf1b1811d8d7b8f44b7f007"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
