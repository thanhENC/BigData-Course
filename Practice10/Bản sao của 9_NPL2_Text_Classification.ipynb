{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dccc7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+\n",
      "|News                                                                                                                                                                                                                                               |Sentiment|\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+\n",
      "|- BEIJING XFN-ASIA - Hong Kong-listed Standard Chartered Bank said it has signed a China mobile phone dealer financing agreement with Nokia , making it the first foreign bank to offer financing to the country 's small and medium enterprise -LR|1        |\n",
      "|- Operating profit rose by 26.9 % to EUR 105.8 ( 83.4 ) million .                                                                                                                                                                                  |1        |\n",
      "|- Provides summary of the medical equipment pipeline products that the company is developing .                                                                                                                                                     |0        |\n",
      "|- So , the sales growth of cars considerably influence on the tires market '' .                                                                                                                                                                    |0        |\n",
      "|- UPM-Kymmene upgraded to ` in-line ' from ` underperform ' by Goldman Sachs .                                                                                                                                                                     |1        |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('9_NPL2_Text_Classification').getOrCreate()\n",
    "\n",
    "file_location = \"9_financial_news.csv\"\n",
    "file_type = \"csv\"\n",
    "infer_schema = \"false\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "text_df = spark.read.format(file_type) \\\n",
    " .option(\"inferSchema\", infer_schema) \\\n",
    " .option(\"header\", first_row_is_header) \\\n",
    " .option(\"sep\", delimiter) \\\n",
    " .load(file_location)\n",
    "text_df.show(5,False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13746750",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "962"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40a7d0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|Sentiment|count|\n",
      "+---------+-----+\n",
      "|        0|  177|\n",
      "|        1|  785|\n",
      "+---------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "text_df=text_df.filter(((text_df.Sentiment =='0') | (text_df.Sentiment =='1') ) )\n",
    "text_df.groupBy('Sentiment').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "075a473c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df=text_df.withColumn(\"Label\",text_df.Sentiment.cast('float')).drop('Sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dbba7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|News                                                                                                                                                                                                                                                                         |Label|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|Revenue grew 12 percent to (  x20ac ) 3.6 billion ( US$ 4.5 billion ) .                                                                                                                                                                                                      |1.0  |\n",
      "|To our members and partners , the use of IT will mostly be apparent in the increased efficiency of the results service , '' observes Perttu Puro from Tradeka .                                                                                                              |1.0  |\n",
      "|`` UPM 's deliveries increased during the third quarter by 4 percent , and the efficiency of operations improved , '' Chief Executive Jussi Pesonen said .                                                                                                                   |1.0  |\n",
      "|Componenta increased its stake in Turkish steel company Doktas Dokumculuk Ticaret ve Sanayi A.S. to 92.6 pct stake in March 2007 .                                                                                                                                           |1.0  |\n",
      "|Patrizia adds the acquisition to a portfolio already worth EUR3bn .                                                                                                                                                                                                          |0.0  |\n",
      "|Finnish Okmetic that manufactures and processes silicon wafers for the semiconductor and sensor industries and Norwegian solar wafer company NorSun have signed a contract under which Okmetic will supply NorSun mono silicon crystals for use in solar cell manufacturing .|1.0  |\n",
      "|`` Our customers now have the chance to make bookings for all the services they want at one location , '' said Mikko Tuomainen , in-charge of Finnair 's Internet ( sales ) .                                                                                                |1.0  |\n",
      "|Earnings per share EPS are seen at EUR 0.56 , up from EUR 0.38 .                                                                                                                                                                                                             |1.0  |\n",
      "|Operating profit rose to EUR 27.8 mn from EUR 17.5 mn in 2008 .                                                                                                                                                                                                              |1.0  |\n",
      "|With the extension of the new contract for a further eight engines , the plant will now produce over 100 MW , it said .                                                                                                                                                      |1.0  |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rand\n",
    "text_df.orderBy(rand()).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd2eb3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------+\n",
      "|                News|Label|length|\n",
      "+--------------------+-----+------+\n",
      "|21 October 2010 -...|  1.0|   212|\n",
      "|With this subscri...|  0.0|   120|\n",
      "|When the OMX mark...|  0.0|   194|\n",
      "|A maximum of 666,...|  0.0|   120|\n",
      "|In Q1 of 2010 , B...|  1.0|    95|\n",
      "|Earnings per shar...|  1.0|    64|\n",
      "|With this appoint...|  1.0|   156|\n",
      "|Kalnapilio-Tauro ...|  1.0|   242|\n",
      "|Finland 's Poyry ...|  1.0|   185|\n",
      "|- UPM-Kymmene upg...|  1.0|    78|\n",
      "+--------------------+-----+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import length\n",
    "text_df=text_df.withColumn('length',length(text_df['News']))\n",
    "text_df.orderBy(rand()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca5f9f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|Label|       avg(Length)|\n",
      "+-----+------------------+\n",
      "|  1.0|134.15031847133758|\n",
      "|  0.0| 142.5084745762712|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_df.groupBy('Label').agg({'Length':'mean'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "294f7543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------+--------------------+\n",
      "|                News|Label|length|              tokens|\n",
      "+--------------------+-----+------+--------------------+\n",
      "|- BEIJING XFN-ASI...|  1.0|   243|[-, beijing, xfn-...|\n",
      "|- Operating profi...|  1.0|    65|[-, operating, pr...|\n",
      "|- Provides summar...|  0.0|    94|[-, provides, sum...|\n",
      "|- So , the sales ...|  0.0|    79|[-, so, ,, the, s...|\n",
      "|- UPM-Kymmene upg...|  1.0|    78|[-, upm-kymmene, ...|\n",
      "|( ADP News ) - De...|  1.0|   231|[(, adp, news, ),...|\n",
      "|( ADP News ) - Fe...|  1.0|   209|[(, adp, news, ),...|\n",
      "|( ADP News ) - Fe...|  1.0|   187|[(, adp, news, ),...|\n",
      "|( ADP News ) - Fe...|  1.0|   200|[(, adp, news, ),...|\n",
      "|( ADP News ) - Fe...|  1.0|   194|[(, adp, news, ),...|\n",
      "|( ADP News ) - No...|  1.0|   231|[(, adp, news, ),...|\n",
      "|( ADP News ) - Oc...|  1.0|   231|[(, adp, news, ),...|\n",
      "|( ADP News ) - Oc...|  1.0|   231|[(, adp, news, ),...|\n",
      "|( ADP News ) - Oc...|  1.0|   231|[(, adp, news, ),...|\n",
      "|( ADP News ) - Se...|  1.0|   239|[(, adp, news, ),...|\n",
      "|( Filippova ) A t...|  1.0|   223|[(, filippova, ),...|\n",
      "|` By separating s...|  1.0|    99|[`, by, separatin...|\n",
      "|` It is a testame...|  0.0|   111|[`, it, is, a, te...|\n",
      "|` Our strategic c...|  1.0|   198|[`, our, strategi...|\n",
      "|` Patja has worke...|  0.0|    96|[`, patja, has, w...|\n",
      "+--------------------+-----+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start the tokenization process\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "tokenization=Tokenizer(inputCol='News',outputCol='tokens')\n",
    "tokenized_df=tokenization.transform(text_df)\n",
    "tokenized_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86520c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------+--------------------+--------------------+\n",
      "|                News|Label|length|              tokens|      refined_tokens|\n",
      "+--------------------+-----+------+--------------------+--------------------+\n",
      "|- BEIJING XFN-ASI...|  1.0|   243|[-, beijing, xfn-...|[-, beijing, xfn-...|\n",
      "|- Operating profi...|  1.0|    65|[-, operating, pr...|[-, operating, pr...|\n",
      "|- Provides summar...|  0.0|    94|[-, provides, sum...|[-, provides, sum...|\n",
      "|- So , the sales ...|  0.0|    79|[-, so, ,, the, s...|[-, ,, sales, gro...|\n",
      "|- UPM-Kymmene upg...|  1.0|    78|[-, upm-kymmene, ...|[-, upm-kymmene, ...|\n",
      "|( ADP News ) - De...|  1.0|   231|[(, adp, news, ),...|[(, adp, news, ),...|\n",
      "|( ADP News ) - Fe...|  1.0|   209|[(, adp, news, ),...|[(, adp, news, ),...|\n",
      "|( ADP News ) - Fe...|  1.0|   187|[(, adp, news, ),...|[(, adp, news, ),...|\n",
      "|( ADP News ) - Fe...|  1.0|   200|[(, adp, news, ),...|[(, adp, news, ),...|\n",
      "|( ADP News ) - Fe...|  1.0|   194|[(, adp, news, ),...|[(, adp, news, ),...|\n",
      "+--------------------+-----+------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# remove stopwords\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "stopword_removal=StopWordsRemover(inputCol='tokens',outputCol='refined_tokens')\n",
    "refined_text_df=stopword_removal.transform(tokenized_df)\n",
    "refined_text_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83575a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 15:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------+--------------------+--------------------+-----------+\n",
      "|                News|Label|length|              tokens|      refined_tokens|token_count|\n",
      "+--------------------+-----+------+--------------------+--------------------+-----------+\n",
      "|Lifetree was foun...|  1.0|   107|[lifetree, was, f...|[lifetree, founde...|         13|\n",
      "|The mill has long...|  1.0|    79|[the, mill, has, ...|[mill, long, trad...|          8|\n",
      "|TELE2 Affarsvarld...|  1.0|   145|[tele2, affarsvar...|[tele2, affarsvar...|         24|\n",
      "|Vaisala also said...|  1.0|   121|[vaisala, also, s...|[vaisala, also, s...|         18|\n",
      "|In its financial ...|  1.0|   197|[in, its, financi...|[financial, repor...|         29|\n",
      "|Raute Corporation...|  1.0|   100|[raute, corporati...|[raute, corporati...|         14|\n",
      "|Finnish sports eq...|  1.0|   232|[finnish, sports,...|[finnish, sports,...|         36|\n",
      "|The value of the ...|  1.0|    68|[the, value, of, ...|[value, firm, 's,...|         10|\n",
      "|We hope to increa...|  1.0|    84|[we, hope, to, in...|[hope, increase, ...|         10|\n",
      "|TELECOMWORLDWIRE-...|  0.0|   236|[telecomworldwire...|[telecomworldwire...|         31|\n",
      "+--------------------+-----+------+--------------------+--------------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create another column (token count) that gives the number of tokens in each row\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import *\n",
    "len_udf = udf(lambda s: len(s), IntegerType())\n",
    "refined_text_df = refined_text_df.withColumn(\"token_count\",len_udf(col('refined_tokens')))\n",
    "refined_text_df.orderBy(rand()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f06c2769",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------------------+-----+\n",
      "|      refined_tokens|token_count|            features|Label|\n",
      "+--------------------+-----------+--------------------+-----+\n",
      "|[-, beijing, xfn-...|         31|(4035,[1,3,14,27,...|  1.0|\n",
      "|[-, operating, pr...|         13|(4035,[0,2,7,8,9,...|  1.0|\n",
      "|[-, provides, sum...|         10|(4035,[0,6,27,70,...|  0.0|\n",
      "|[-, ,, sales, gro...|         11|(4035,[0,1,5,27,3...|  0.0|\n",
      "|[-, upm-kymmene, ...|         12|(4035,[0,27,60,15...|  1.0|\n",
      "|[(, adp, news, ),...|         39|(4035,[1,2,6,8,9,...|  1.0|\n",
      "|[(, adp, news, ),...|         41|(4035,[0,1,2,8,9,...|  1.0|\n",
      "|[(, adp, news, ),...|         38|(4035,[0,1,2,8,9,...|  1.0|\n",
      "|[(, adp, news, ),...|         41|(4035,[0,1,2,6,8,...|  1.0|\n",
      "|[(, adp, news, ),...|         38|(4035,[0,1,2,6,8,...|  1.0|\n",
      "+--------------------+-----------+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Convert text into numerical features:\n",
    "# Use CountVectorizer for feature vectorization for the ML \n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "count_vec=CountVectorizer(inputCol='refined_tokens',outputCol='features')\n",
    "cv_text_df=count_vec.fit(refined_text_df).transform(refined_text_df)\n",
    "cv_text_df.select(['refined_tokens','token_count','features','Label']).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe3310bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+-----+\n",
      "|features                                                                                                                                                                                                                                                                  |token_count|Label|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+-----+\n",
      "|(4035,[1,3,14,27,34,43,44,68,71,97,119,152,429,451,533,828,830,969,1167,1187,1270,1297,1401,2132,2199,2230,2857,3960],[1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                  |31         |1.0  |\n",
      "|(4035,[0,2,7,8,9,11,13,18,19,27,1308,2672,3338],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                                                                    |13         |1.0  |\n",
      "|(4035,[0,6,27,70,85,195,810,1430,1939,3094],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                                                                                    |10         |0.0  |\n",
      "|(4035,[0,1,5,27,36,45,53,711,1000,1688,3711],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                                                                               |11         |0.0  |\n",
      "|(4035,[0,27,60,150,397,1537,2397,2552,2913,3808],[1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                                                                               |12         |1.0  |\n",
      "|(4035,[1,2,6,8,9,12,13,14,16,25,27,30,31,48,57,78,83,94,108,120,140,161,164,305,310,482,702,1809,1971,2384,2458,2854,2946,3343],[1.0,1.0,1.0,3.0,3.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|39         |1.0  |\n",
      "|(4035,[0,1,2,8,9,10,11,12,13,14,16,19,22,25,27,30,31,41,48,58,78,87,108,139,142,162,164,289,305,368,554,585,1491,3684],[1.0,1.0,2.0,3.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])         |41         |1.0  |\n",
      "|(4035,[0,1,2,8,9,10,11,12,13,14,19,22,25,27,30,31,41,48,58,78,87,108,142,156,164,167,585,772,909,1478,2666],[1.0,1.0,2.0,3.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                |38         |1.0  |\n",
      "|(4035,[0,1,2,6,8,9,10,11,12,13,14,16,19,22,25,27,31,41,48,58,78,108,164,377,467,585,614,871,921,1104,1131,1397,1966,2879],[1.0,1.0,2.0,1.0,3.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])      |41         |1.0  |\n",
      "|(4035,[0,1,2,6,8,9,10,12,13,14,22,25,27,30,31,41,47,48,55,58,78,108,113,164,421,585,1020,1138,1216,1795,2158,3782],[1.0,1.0,2.0,1.0,3.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                     |38         |1.0  |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 21:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model_text_df=cv_text_df.select(['features','token_count','Label'])\n",
    "model_text_df.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2615f6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- token_count: integer (nullable = true)\n",
      " |-- Label: float (nullable = true)\n",
      " |-- features_vec: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use VectorAssembler to create input features for the Machine Learning model:\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "df_assembler = VectorAssembler(inputCols=['features','token_count'],outputCol='features_vec')\n",
    "model_text_df = df_assembler.transform(model_text_df)\n",
    "model_text_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e85c8491",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 22:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+-----+--------------------+\n",
      "|            features|token_count|Label|        features_vec|\n",
      "+--------------------+-----------+-----+--------------------+\n",
      "|(4035,[0,1,2,3,4,...|         20|  1.0|(4036,[0,1,2,3,4,...|\n",
      "|(4035,[0,1,2,3,4,...|         20|  1.0|(4036,[0,1,2,3,4,...|\n",
      "|(4035,[0,1,2,3,4,...|         17|  1.0|(4036,[0,1,2,3,4,...|\n",
      "|(4035,[0,1,2,3,4,...|         19|  1.0|(4036,[0,1,2,3,4,...|\n",
      "|(4035,[0,1,2,3,4,...|         18|  1.0|(4036,[0,1,2,3,4,...|\n",
      "|(4035,[0,1,2,3,4,...|         21|  1.0|(4036,[0,1,2,3,4,...|\n",
      "|(4035,[0,1,2,3,4,...|         18|  1.0|(4036,[0,1,2,3,4,...|\n",
      "|(4035,[0,1,2,3,4,...|         17|  1.0|(4036,[0,1,2,3,4,...|\n",
      "|(4035,[0,1,2,3,5,...|         16|  1.0|(4036,[0,1,2,3,5,...|\n",
      "|(4035,[0,1,2,3,5,...|         16|  1.0|(4036,[0,1,2,3,5,...|\n",
      "+--------------------+-----------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# proceed with training a logistic regression model:\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "training_df,test_df=model_text_df.randomSplit([0.75,0.25])\n",
    "training_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9697b407",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 23:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|Label|count|\n",
      "+-----+-----+\n",
      "|  1.0|  579|\n",
      "|  0.0|  139|\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "training_df.groupBy('Label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c0d74eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 26:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|Label|count|\n",
      "+-----+-----+\n",
      "|  1.0|  206|\n",
      "|  0.0|   38|\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test_df.groupBy('Label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a471c696",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/28 16:48:15 ERROR LBFGS: Failure! Resetting history: breeze.optimize.StepSizeUnderflow: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 193:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+-----+--------------------+--------------------+--------------------+----------+\n",
      "|            features|token_count|Label|        features_vec|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----------+-----+--------------------+--------------------+--------------------+----------+\n",
      "|(4035,[0,1,2,3,4,...|         24|  1.0|(4036,[0,1,2,3,4,...|[-24.078963252882...|[3.48850322902546...|       1.0|\n",
      "|(4035,[0,1,2,3,4,...|         15|  1.0|(4036,[0,1,2,3,4,...|[-13.709397126724...|[1.11194676256611...|       1.0|\n",
      "|(4035,[0,1,2,3,6,...|         24|  0.0|(4036,[0,1,2,3,6,...|[5.91092800688817...|[0.99729765134933...|       0.0|\n",
      "|(4035,[0,1,2,3,10...|         17|  1.0|(4036,[0,1,2,3,10...|[-19.068883455673...|[5.22984884454914...|       1.0|\n",
      "|(4035,[0,1,2,3,17...|         34|  1.0|(4036,[0,1,2,3,17...|[-12.518715830999...|[3.65754102094011...|       1.0|\n",
      "|(4035,[0,1,2,3,26...|         16|  0.0|(4036,[0,1,2,3,26...|[22.6285715263782...|[0.99999999985122...|       0.0|\n",
      "|(4035,[0,1,2,4,5,...|         21|  1.0|(4036,[0,1,2,4,5,...|[-21.996233497817...|[2.79999444105837...|       1.0|\n",
      "|(4035,[0,1,2,4,5,...|         20|  1.0|(4036,[0,1,2,4,5,...|[-14.933559232805...|[3.26916987307453...|       1.0|\n",
      "|(4035,[0,1,2,4,5,...|         34|  1.0|(4036,[0,1,2,4,5,...|[-28.086452776930...|[6.34174160602570...|       1.0|\n",
      "|(4035,[0,1,2,4,5,...|         26|  1.0|(4036,[0,1,2,4,5,...|[2.74492118221617...|[0.93962587560192...|       0.0|\n",
      "+--------------------+-----------+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "LR_Model=LogisticRegression(featuresCol='features_vec',labelCol='Label').fit(training_df)\n",
    "predictions=LR_Model.evaluate(test_df).predictions\n",
    "predictions.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63c1dc28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 195:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+----------+\n",
      "|            features|label|         probability|prediction|\n",
      "+--------------------+-----+--------------------+----------+\n",
      "|(4035,[0,1,2,3,4,...|  1.0|[3.48850322902546...|       1.0|\n",
      "|(4035,[0,1,2,3,4,...|  1.0|[1.11194676256611...|       1.0|\n",
      "|(4035,[0,1,2,3,6,...|  0.0|[0.99729765134933...|       0.0|\n",
      "|(4035,[0,1,2,3,10...|  1.0|[5.22984884454914...|       1.0|\n",
      "|(4035,[0,1,2,3,17...|  1.0|[3.65754102094011...|       1.0|\n",
      "+--------------------+-----+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Evaluate the performance of the logistic regression model using accuracy metrics on test data.\n",
    "predictions.select(\"features\", \"label\", \"probability\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a0fc09a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|label|count(label)|\n",
      "+-----+------------+\n",
      "|  1.0|         206|\n",
      "|  0.0|          38|\n",
      "+-----+------------+\n",
      "\n",
      "+----------+-----------------+\n",
      "|prediction|count(prediction)|\n",
      "+----------+-----------------+\n",
      "|       0.0|               44|\n",
      "|       1.0|              200|\n",
      "+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute the number of class in the label and the prediction:\n",
    "M = predictions.select(\"label\", \"prediction\")          \n",
    "M.groupby('label').agg({'label': 'count'}).show()  \n",
    "M.groupby('prediction').agg({'prediction': 'count'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a8297d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 265:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  1.0|       0.0|   22|\n",
      "|  0.0|       0.0|   22|\n",
      "|  1.0|       1.0|  184|\n",
      "|  0.0|       1.0|   16|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions.groupBy('label', 'prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ae6a8cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8442622950819673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 494:>                                                        (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "Accuracy = M.filter(M.label == M.prediction).count()/M.count()\n",
    "print(\"Accuracy:\",Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "96f17692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Evaluation Metrics----\n",
      "Accuracy: 0.8442622950819673\n",
      "Precision: 0.92\n",
      "Recall: 0.8932038834951457\n",
      "Area under PR: 0.9159557536208818\n",
      "Area under ROC: 0.7360756259580992\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "predictionAndLabel = predictions.select(\"prediction\", \"Label\").rdd\n",
    "\n",
    "# Instantiate metrics object \n",
    "metricsMulti = MulticlassMetrics(predictionAndLabel)\n",
    "metricsBinary= BinaryClassificationMetrics(predictionAndLabel)\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Label\", \n",
    "                                              predictionCol=\"prediction\", \n",
    "                                              metricName=\"accuracy\")\n",
    "\n",
    "# Overall statistics \n",
    "precision = metricsMulti.precision(label=1) \n",
    "recall = metricsMulti.recall(label=1) \n",
    "\n",
    "print(\"---Evaluation Metrics----\")\n",
    "print(\"Accuracy:\",format(evaluator.evaluate(predictions)))\n",
    "print(\"Precision:\", precision) \n",
    "print(\"Recall:\", recall)\n",
    "      \n",
    "# Area under precision-recall curve \n",
    "print(\"Area under PR:\",metricsBinary.areaUnderPR) \n",
    "# Area under ROC curve \n",
    "print(\"Area under ROC:\",metricsBinary.areaUnderROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b7ae9dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 556:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+-----+--------------------+--------------------+--------------------+----------+\n",
      "|            features|token_count|Label|        features_vec|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----------+-----+--------------------+--------------------+--------------------+----------+\n",
      "|(4035,[0,1,2,3,4,...|         24|  1.0|(4036,[0,1,2,3,4,...|[2.91698764482767...|[0.14584938224138...|       1.0|\n",
      "|(4035,[0,1,2,3,4,...|         15|  1.0|(4036,[0,1,2,3,4,...|[3.11237226021228...|[0.15561861301061...|       1.0|\n",
      "|(4035,[0,1,2,3,6,...|         24|  0.0|(4036,[0,1,2,3,6,...|[3.51774214969079...|[0.17588710748453...|       1.0|\n",
      "|(4035,[0,1,2,3,10...|         17|  1.0|(4036,[0,1,2,3,10...|[3.23706206129527...|[0.16185310306476...|       1.0|\n",
      "|(4035,[0,1,2,3,17...|         34|  1.0|(4036,[0,1,2,3,17...|[4.07771621495986...|[0.20388581074799...|       1.0|\n",
      "|(4035,[0,1,2,3,26...|         16|  0.0|(4036,[0,1,2,3,26...|[5.44029228606349...|[0.27201461430317...|       1.0|\n",
      "|(4035,[0,1,2,4,5,...|         21|  1.0|(4036,[0,1,2,4,5,...|[2.97287608387400...|[0.14864380419370...|       1.0|\n",
      "|(4035,[0,1,2,4,5,...|         20|  1.0|(4036,[0,1,2,4,5,...|[3.05242153841945...|[0.15262107692097...|       1.0|\n",
      "|(4035,[0,1,2,4,5,...|         34|  1.0|(4036,[0,1,2,4,5,...|[3.33310162681497...|[0.16665508134074...|       1.0|\n",
      "|(4035,[0,1,2,4,5,...|         26|  1.0|(4036,[0,1,2,4,5,...|[2.97287608387400...|[0.14864380419370...|       1.0|\n",
      "+--------------------+-----------+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "RF_Model=RandomForestClassifier(featuresCol='features_vec',labelCol='Label').fit(training_df)\n",
    "predictions=RF_Model.evaluate(test_df).predictions\n",
    "predictions.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "705d5274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+----------+\n",
      "|            features|label|         probability|prediction|\n",
      "+--------------------+-----+--------------------+----------+\n",
      "|(4035,[0,1,2,3,4,...|  1.0|[0.14584938224138...|       1.0|\n",
      "|(4035,[0,1,2,3,4,...|  1.0|[0.15561861301061...|       1.0|\n",
      "|(4035,[0,1,2,3,6,...|  0.0|[0.17588710748453...|       1.0|\n",
      "|(4035,[0,1,2,3,10...|  1.0|[0.16185310306476...|       1.0|\n",
      "|(4035,[0,1,2,3,17...|  1.0|[0.20388581074799...|       1.0|\n",
      "+--------------------+-----+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the performance of the Random Forest model using accuracy metrics on test data.\n",
    "predictions.select(\"features\", \"label\", \"probability\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "db79b365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|label|count(label)|\n",
      "+-----+------------+\n",
      "|  1.0|         206|\n",
      "|  0.0|          38|\n",
      "+-----+------------+\n",
      "\n",
      "+----------+-----------------+\n",
      "|prediction|count(prediction)|\n",
      "+----------+-----------------+\n",
      "|       1.0|              244|\n",
      "+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute the number of class in the label and the prediction:\n",
    "M = predictions.select(\"label\", \"prediction\")          \n",
    "M.groupby('label').agg({'label': 'count'}).show()  \n",
    "M.groupby('prediction').agg({'prediction': 'count'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "02c13122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  1.0|       1.0|  206|\n",
      "|  0.0|       1.0|   38|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.groupBy('label', 'prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4ef37291",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 567:>                                                        (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8442622950819673\n"
     ]
    }
   ],
   "source": [
    "Accuracy = M.filter(M.label == M.prediction).count()/M.count()\n",
    "print(\"Accuracy:\",Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "aeccc354",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 575:>                                                        (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Evaluation Metrics----\n",
      "Accuracy: 0.8442622950819673\n",
      "Precision: 0.8442622950819673\n",
      "Recall: 1.0\n",
      "Area under PR: 0.8442622950819673\n",
      "Area under ROC: 0.5\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "predictionAndLabel = predictions.select(\"prediction\", \"Label\").rdd\n",
    "\n",
    "# Instantiate metrics object \n",
    "metricsMulti = MulticlassMetrics(predictionAndLabel)\n",
    "metricsBinary= BinaryClassificationMetrics(predictionAndLabel)\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Label\", \n",
    "                                              predictionCol=\"prediction\", \n",
    "                                              metricName=\"accuracy\")\n",
    "\n",
    "# Overall statistics \n",
    "precision = metricsMulti.precision(label=1) \n",
    "recall = metricsMulti.recall(label=1) \n",
    "\n",
    "print(\"---Evaluation Metrics----\")\n",
    "print(\"Accuracy:\",format(evaluator.evaluate(predictions)))\n",
    "print(\"Precision:\", precision) \n",
    "print(\"Recall:\", recall)\n",
    "      \n",
    "# Area under precision-recall curve \n",
    "print(\"Area under PR:\",metricsBinary.areaUnderPR) \n",
    "# Area under ROC curve \n",
    "print(\"Area under ROC:\",metricsBinary.areaUnderROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e8bebfc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+-----+--------------------+--------------------+--------------------+----------+\n",
      "|            features|token_count|Label|        features_vec|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----------+-----+--------------------+--------------------+--------------------+----------+\n",
      "|(4035,[0,1,2,3,4,...|         24|  1.0|(4036,[0,1,2,3,4,...|[-1.2498224622239...|[0.07588307588621...|       1.0|\n",
      "|(4035,[0,1,2,3,4,...|         15|  1.0|(4036,[0,1,2,3,4,...|[-1.2498224622239...|[0.07588307588621...|       1.0|\n",
      "|(4035,[0,1,2,3,6,...|         24|  0.0|(4036,[0,1,2,3,6,...|[-0.7753704067183...|[0.17497929744182...|       1.0|\n",
      "|(4035,[0,1,2,3,10...|         17|  1.0|(4036,[0,1,2,3,10...|[-1.0012518184503...|[0.11894030671609...|       1.0|\n",
      "|(4035,[0,1,2,3,17...|         34|  1.0|(4036,[0,1,2,3,17...|[-0.7753704067183...|[0.17497929744182...|       1.0|\n",
      "|(4035,[0,1,2,3,26...|         16|  0.0|(4036,[0,1,2,3,26...|[1.06682040756522...|[0.89413014795718...|       0.0|\n",
      "|(4035,[0,1,2,4,5,...|         21|  1.0|(4036,[0,1,2,4,5,...|[-1.2498224622239...|[0.07588307588621...|       1.0|\n",
      "|(4035,[0,1,2,4,5,...|         20|  1.0|(4036,[0,1,2,4,5,...|[-0.7753704067183...|[0.17497929744182...|       1.0|\n",
      "|(4035,[0,1,2,4,5,...|         34|  1.0|(4036,[0,1,2,4,5,...|[-1.0012518184503...|[0.11894030671609...|       1.0|\n",
      "|(4035,[0,1,2,4,5,...|         26|  1.0|(4036,[0,1,2,4,5,...|[-1.2498224622239...|[0.07588307588621...|       1.0|\n",
      "+--------------------+-----------+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "GB_Model=GBTClassifier(featuresCol='features_vec',labelCol='Label',maxIter=10).fit(training_df)\n",
    "predictions = GB_Model.transform(test_df)\n",
    "predictions.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "dfdaa6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+----------+\n",
      "|            features|label|         probability|prediction|\n",
      "+--------------------+-----+--------------------+----------+\n",
      "|(4035,[0,1,2,3,4,...|  1.0|[0.07588307588621...|       1.0|\n",
      "|(4035,[0,1,2,3,4,...|  1.0|[0.07588307588621...|       1.0|\n",
      "|(4035,[0,1,2,3,6,...|  0.0|[0.17497929744182...|       1.0|\n",
      "|(4035,[0,1,2,3,10...|  1.0|[0.11894030671609...|       1.0|\n",
      "|(4035,[0,1,2,3,17...|  1.0|[0.17497929744182...|       1.0|\n",
      "+--------------------+-----+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1315:>                                                       (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Evaluate the performance of the Gradient-boosted tree model using accuracy metrics on test data.\n",
    "predictions.select(\"features\", \"label\", \"probability\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ef798d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|label|count(label)|\n",
      "+-----+------------+\n",
      "|  1.0|         206|\n",
      "|  0.0|          38|\n",
      "+-----+------------+\n",
      "\n",
      "+----------+-----------------+\n",
      "|prediction|count(prediction)|\n",
      "+----------+-----------------+\n",
      "|       0.0|               14|\n",
      "|       1.0|              230|\n",
      "+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute the number of class in the label and the prediction:\n",
    "M = predictions.select(\"label\", \"prediction\")          \n",
    "M.groupby('label').agg({'label': 'count'}).show()  \n",
    "M.groupby('prediction').agg({'prediction': 'count'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "76826a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  1.0|       0.0|    5|\n",
      "|  0.0|       0.0|    9|\n",
      "|  1.0|       1.0|  201|\n",
      "|  0.0|       1.0|   29|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.groupBy('label', 'prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8806d093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.860655737704918\n"
     ]
    }
   ],
   "source": [
    "Accuracy = M.filter(M.label == M.prediction).count()/M.count()\n",
    "print(\"Accuracy:\",Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e5cbefb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadoopuser/anaconda3/lib/python3.9/site-packages/pyspark/sql/context.py:157: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Evaluation Metrics----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.860655737704918\n",
      "Precision: 0.8739130434782608\n",
      "Recall: 0.9757281553398058\n",
      "Area under PR: 0.8735532042986942\n",
      "Area under ROC: 0.6062851303014819\n",
      "22/10/29 06:58:43 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 46139359 ms exceeds timeout 120000 ms\n",
      "22/10/29 06:58:43 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "predictionAndLabel = predictions.select(\"prediction\", \"Label\").rdd\n",
    "\n",
    "# Instantiate metrics object \n",
    "metricsMulti = MulticlassMetrics(predictionAndLabel)\n",
    "metricsBinary= BinaryClassificationMetrics(predictionAndLabel)\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Label\", \n",
    "                                              predictionCol=\"prediction\", \n",
    "                                              metricName=\"accuracy\")\n",
    "\n",
    "# Overall statistics \n",
    "precision = metricsMulti.precision(label=1) \n",
    "recall = metricsMulti.recall(label=1) \n",
    "\n",
    "print(\"---Evaluation Metrics----\")\n",
    "print(\"Accuracy:\",format(evaluator.evaluate(predictions)))\n",
    "print(\"Precision:\", precision) \n",
    "print(\"Recall:\", recall)\n",
    "      \n",
    "# Area under precision-recall curve \n",
    "print(\"Area under PR:\",metricsBinary.areaUnderPR) \n",
    "# Area under ROC curve \n",
    "print(\"Area under ROC:\",metricsBinary.areaUnderROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456f32b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
